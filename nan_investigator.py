#!/usr/bin/env python3
"""
Investigate NaN sources in FLUX LoRA training
"""

import torch
import os
import sys
from pathlib import Path

# Add sd-scripts to path
sys.path.insert(0, '/workspace/music-video-generator/sd-scripts')

def test_flux_lora_network():
    """Test if the LoRA network itself produces NaNs"""
    print("üî¨ Testing FLUX LoRA Network Initialization...")
    
    try:
        from networks.lora_flux import LoRANetwork
        
        # Create minimal network
        network = LoRANetwork(
            text_encoder=None,
            unet=None,
            multiplier=1.0,
            lora_dim=8,
            alpha=4,
            dropout=None,
            rank_dropout=None,
            module_dropout=None,
            use_cp=False,
            conv_lora_dim=None,
            conv_alpha=None,
            block_dims=None,
            block_alphas=None,
            train_norm=False,
            varbose=False
        )
        
        print("‚úÖ LoRA network created successfully")
        
        # Check if any weights are NaN immediately after initialization
        nan_found = False
        for name, param in network.named_parameters():
            if torch.isnan(param).any():
                print(f"‚ùå NaN found in {name} after initialization!")
                nan_found = True
            else:
                print(f"‚úÖ {name}: Mean = {param.mean().item():.6f}")
        
        if not nan_found:
            print("‚úÖ All network weights initialized correctly")
        
        return network
        
    except Exception as e:
        print(f"‚ùå Error creating LoRA network: {e}")
        return None

def test_optimizer_compatibility():
    """Test if AdamW optimizer causes issues with bf16"""
    print("\nüî¨ Testing Optimizer with bf16...")
    
    # Create simple tensor
    x = torch.randn(10, 10, dtype=torch.bfloat16, requires_grad=True)
    print(f"‚úÖ Test tensor: Mean = {x.float().mean().item():.6f}")
    
    # Test AdamW
    optimizer = torch.optim.AdamW([x], lr=2e-5)
    print("‚úÖ AdamW optimizer created")
    
    # Simulate training step
    loss = (x ** 2).sum()
    print(f"‚úÖ Loss: {loss.item():.6f}")
    
    loss.backward()
    print(f"‚úÖ Gradient: Mean = {x.grad.float().mean().item():.6f}")
    
    optimizer.step()
    print(f"‚úÖ After step: Mean = {x.float().mean().item():.6f}")
    
    if torch.isnan(x).any():
        print("‚ùå NaN detected after optimizer step!")
        return False
    else:
        print("‚úÖ Optimizer step successful")
        return True

def test_mixed_precision():
    """Test if mixed precision training causes NaNs"""
    print("\nüî¨ Testing Mixed Precision...")
    
    # Test with GradScaler
    scaler = torch.cuda.amp.GradScaler()
    
    x = torch.randn(10, 10, dtype=torch.float32, requires_grad=True, device='cuda' if torch.cuda.is_available() else 'cpu')
    optimizer = torch.optim.AdamW([x], lr=2e-5)
    
    print(f"‚úÖ Initial: Mean = {x.mean().item():.6f}")
    
    with torch.cuda.amp.autocast(dtype=torch.bfloat16):
        loss = (x ** 2).sum()
        print(f"‚úÖ Loss (autocast): {loss.item():.6f}")
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
    
    if torch.isnan(x).any():
        print("‚ùå NaN detected after mixed precision step!")
        return False
    else:
        print(f"‚úÖ After mixed precision: Mean = {x.mean().item():.6f}")
        return True

def test_large_learning_rates():
    """Test if learning rate is too high"""
    print("\nüî¨ Testing Learning Rate Sensitivity...")
    
    learning_rates = [1e-6, 1e-5, 2e-5, 1e-4, 1e-3]
    
    for lr in learning_rates:
        x = torch.randn(10, 10, dtype=torch.bfloat16, requires_grad=True)
        optimizer = torch.optim.AdamW([x], lr=lr)
        
        # Multiple steps to see if NaN develops
        for step in range(5):
            loss = (x ** 2).sum()
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            
            if torch.isnan(x).any():
                print(f"‚ùå NaN at LR {lr} after step {step+1}")
                break
        else:
            print(f"‚úÖ LR {lr}: Stable after 5 steps, Mean = {x.float().mean().item():.6f}")

def main():
    print("üß™ FLUX LoRA NaN Investigation")
    print("=" * 50)
    
    # Test 1: Network initialization
    network = test_flux_lora_network()
    
    # Test 2: Optimizer compatibility
    test_optimizer_compatibility()
    
    # Test 3: Mixed precision
    if torch.cuda.is_available():
        test_mixed_precision()
    else:
        print("‚ö†Ô∏è CUDA not available, skipping mixed precision test")
    
    # Test 4: Learning rate sensitivity
    test_large_learning_rates()
    
    print("\nüéØ Investigation Complete!")
    print("Check the results above to identify potential NaN sources.")

if __name__ == "__main__":
    main()
